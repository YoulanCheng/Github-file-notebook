{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM391Eo/jGpKViIUDji+9Zj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoulanCheng/Github-file-notebook/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5a475db"
      },
      "source": [
        "# Project Title: [Your Project Title Here]\n",
        "\n",
        "**Author:** [Your Name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "090abca3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "OxpS3Ez2VRJZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.iolib.summary2 import summary_col\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from statsmodels.graphics.regressionplots import plot_partregress"
      ],
      "metadata": {
        "id": "lu3_ifYTVS2S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats"
      ],
      "metadata": {
        "id": "oLlQZYd8VWLz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
        "plt.rcParams['figure.figsize'] = (7, 5)"
      ],
      "metadata": {
        "id": "4sWYJaRQVYfY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f11915f1"
      },
      "source": [
        "## 1. Data Acquisition, Vetting, and Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ba45a8c"
      },
      "source": [
        "### 1.1. Research Questions and Hypothesis\n",
        "This project focuses on cross-national differences in life expectancy and asks how environmental and socioeconomic factors jointly shape population health.\n",
        "\n",
        "**Research Question 1 (Q1)**  \n",
        "Does higher exposure to PM2.5 air pollution reduce life expectancy across countries?\n",
        "\n",
        "- **Hypothesis 1 (H1)**  \n",
        "  Holding other factors constant, higher PM2.5 levels are associated with lower life expectancy.  \n",
        "  *Theoretical justification:* Epidemiological and public health research shows that fine particulate matter increases the risk of respiratory and cardiovascular diseases, which directly affect mortality.\n",
        "\n",
        "---\n",
        "\n",
        "**Research Question 2 (Q2)**  \n",
        "Does economic development improve life expectancy, net of air pollution and other controls?\n",
        "\n",
        "- **Hypothesis 2 (H2)**  \n",
        "  Controlling for air pollution and other predictors, higher GDP per capita is associated with higher life expectancy.  \n",
        "  *Theoretical justification:* Classic political economy and development literature links income to better nutrition, housing, sanitation, education, and access to healthcare, all of which contribute to longer lives.\n",
        "\n",
        "---\n",
        "\n",
        "**Research Question 3 (Q3)**  \n",
        "Does health expenditure mitigate the negative effect of PM2.5 on life expectancy?\n",
        "\n",
        "- **Hypothesis 3 (H3)**  \n",
        "  The negative effect of PM2.5 on life expectancy is weaker in countries that spend a higher share of GDP on health.  \n",
        "  *Theoretical justification:* Health systems with more resources can invest in prevention, screening, and treatment, and thus partially shield populations from the health consequences of environmental risks.\n",
        "\n",
        "---\n",
        "\n",
        "**Research Question 4 (Q4)**  \n",
        "How do urbanization and health expenditure, together with pollution and income, shape cross-national differences in life expectancy?\n",
        "\n",
        "- **Hypothesis 4 (H4)**  \n",
        "  Urbanization and health expenditure are positively associated with life expectancy, once pollution and income are controlled for.  \n",
        "  *Theoretical justification:* Urban areas often host better medical infrastructure and services, and health spending reflects state capacity and policy commitment to public health. However, urbanization may also increase exposure to pollution and crowding, so its net effect is theoretically ambiguous."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95d5e452"
      },
      "source": [
        "### 1.2. Dataset Justification\n",
        "<!-- Describe the dataset(s) you are using. Explain the source, the key variables of interest, and justify why this data is appropriate for answering your research question. -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Dataset Justification\n",
        "\n",
        "The analysis uses a cross-sectional dataset constructed from the **World Development Indicators (WDI)** provided by the World Bank for the year **2019**. The dataset `air_health_2019.csv` contains one row per country and includes:\n",
        "\n",
        "- `country`: Country name (identifier).\n",
        "- `LifeExpectancy`: Life expectancy at birth (years).\n",
        "- `PM25`: Annual mean exposure to PM2.5 (µg/m³).\n",
        "- `GDPpc`: GDP per capita in current US dollars.\n",
        "- `UrbanRate`: Urban population as a percentage of total population.\n",
        "- `HealthExp`: Current health expenditure as a percentage of GDP.\n",
        "\n",
        "**Why this dataset is appropriate:**\n",
        "\n",
        "1. **Global coverage and variation**  \n",
        "   The data cover roughly 180 countries, providing substantial variation in environmental conditions, income levels, urbanization, and health spending. This variation is essential for identifying relationships in a cross-national regression framework.\n",
        "\n",
        "2. **Temporal focus (2019)**  \n",
        "   The year 2019 is the last pre-COVID year with broad global coverage. Using 2019 avoids the severe distortions created by the COVID-19 pandemic (e.g., excess mortality, sudden GDP contractions) while still relying on recent data.\n",
        "\n",
        "3. **Conceptual alignment**  \n",
        "   Each variable corresponds directly to a key concept in the research questions:  \n",
        "   - `PM25` → environmental risk;  \n",
        "   - `GDPpc` → economic development;  \n",
        "   - `UrbanRate` → structural transformation in settlement patterns;  \n",
        "   - `HealthExp` → health system investment;  \n",
        "   - `LifeExpectancy` → overall population health outcome.\n",
        "\n",
        "4. **Data quality and comparability**  \n",
        "   The WDI are widely used in political science and public policy research. Indicators are based on standardized definitions, consistent country codes, and documented methodologies. This supports both **reliability** (measurement consistency) and **comparability** across countries.\n",
        "\n",
        "5. **Reproducibility**  \n",
        "   The CSV file used in this notebook can be stored in the same GitHub repository as the notebook. Anyone can clone the repository and reproduce the entire analysis by running the notebook.\n"
      ],
      "metadata": {
        "id": "YKkOLNzcVsLQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855a69e1"
      },
      "source": [
        "### 1.3. Data Loading, Merging and Initial Exploration\n",
        "Provide the code to load your dataset into a pandas DataFrame. Merge your data frames if necessary.  Display the .head(), .info(), and .describe() outputs. Provide a brief markdown interpretation of the initial state of the data (e.g., number of observations, data types, potential missing values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0ad1ba62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "1e83f8b8-721f-4115-b754-8a96e226091e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'air_health_2019.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1001874913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"air_health_2019.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'air_health_2019.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"air_health_2019.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "LMdyseP2WKSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "F-9omDohWJ84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "TYHohMfaWNxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initial Interpretation**\n",
        "\n",
        "1. The dataset contains one observation per country. The main analytical variables (`LifeExpectancy`, `PM25`, `GDPpc`, `UrbanRate`, `HealthExp`) are numerical (`float64`), which is appropriate for multiple linear regression.\n",
        "2. `country` is a string identifier and will not be used directly as a predictor.\n",
        "3. The descriptive statistics indicate plausible ranges: life expectancy between roughly 50 and 85 years, PM2.5 mostly in the single or double digits, and a wide range in GDP per capita, as expected in a global sample.\n",
        "4. Any missing values will be systematically examined and handled in the next section.\n",
        "\n",
        "To better understand the structure of the data, I next inspect missingness patterns and basic distributions."
      ],
      "metadata": {
        "id": "5MeklvcCWRTN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "570f65dd"
      },
      "source": [
        "<!-- Your interpretation here. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3d12562"
      },
      "source": [
        "## 2. Systematic Data Cleaning and Transformation\n",
        "Document and justify every step of your data cleaning and preprocessing. Use sub-sections for clarity (e.g., Handling Missing Values, Creating Dummy Variables, Addressing Outliers). Show your code and explain the rationale behind each significant decision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a3ccb4a"
      },
      "source": [
        "### 2.1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d24ed0a6"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "# print(df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing values\n",
        "# df_clean = df.dropna()\n",
        "\n",
        "# Fill missing values with mean/median\n",
        "# df['column_name'].fillna(df['column_name'].mean(), inplace=True)\n",
        "\n",
        "# Fill missing values with a specific value\n",
        "# df['column_name'].fillna('Unknown', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "F3gw6P8JWdIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df.copy()\n",
        "\n",
        "numeric_cols = [\"LifeExpectancy\", \"PM25\", \"GDPpc\", \"UrbanRate\", \"HealthExp\"]\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if df_clean[col].isna().any():\n",
        "        median_value = df_clean[col].median()\n",
        "        df_clean[col] = df_clean[col].fillna(median_value)\n",
        "        print(f\"Filled missing values in {col} with median = {median_value:.2f}\")"
      ],
      "metadata": {
        "id": "9YQrIObWXUVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_clean.dropna(subset=[\"LifeExpectancy\"])"
      ],
      "metadata": {
        "id": "s-22Gpx5XWEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Justification**\n",
        "\n",
        "1. Missingness in macro-level indicators is usually due to incomplete reporting for specific countries, not to individual-level nonresponse.\n",
        "2. For skewed variables such as `PM25` and `GDPpc`, **median imputation** is more robust than mean imputation because it is less influenced by extreme values.\n",
        "3. Imputing independent variables with their median avoids dropping countries entirely, which helps maintain statistical power and preserve global variation.\n",
        "4. The dependent variable `LifeExpectancy` is not imputed; instead, any remaining missing values are dropped. Imputing the outcome would artificially reduce uncertainty in the regression and complicate interpretation.\n",
        "\n",
        "This approach strikes a balance between preserving observations and maintaining reasonable distributional properties.\n"
      ],
      "metadata": {
        "id": "fIwk_-zmXZAN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fe580ea"
      },
      "source": [
        "### 2.2 Creating Dummy Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f77556fb"
      },
      "outputs": [],
      "source": [
        "# Convert categorical variables to dummy variables\n",
        "# df = pd.get_dummies(df, columns=['categorical_column'], prefix='cat')\n",
        "\n",
        "# Or create dummy variables manually\n",
        "# df['dummy_var'] = df['categorical_column'].map({'Category1': 1, 'Category2': 0})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "europe_list = [\n",
        "    \"Poland\", \"Germany\", \"France\", \"Spain\", \"Italy\", \"United Kingdom\", \"Netherlands\",\n",
        "    \"Belgium\", \"Sweden\", \"Norway\", \"Denmark\", \"Finland\", \"Czech Republic\", \"Austria\",\n",
        "    \"Switzerland\", \"Portugal\", \"Greece\", \"Ireland\", \"Hungary\"\n",
        "]"
      ],
      "metadata": {
        "id": "jax4yAIMXNll"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean[\"RegionSimple\"] = df_clean[\"country\"].apply(\n",
        "    lambda x: \"Europe\" if x in europe_list else \"Other\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "Gy48rm2RXj-e",
        "outputId": "0eb134f3-e7a4-4a97-cd15-9758eb97edc9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_clean' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-105343857.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df_clean[\"RegionSimple\"] = df_clean[\"country\"].apply(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Europe\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meurope_list\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"Other\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_clean' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = pd.get_dummies(df_clean, columns=[\"RegionSimple\"], drop_first=True)"
      ],
      "metadata": {
        "id": "ew_7joRSXngt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean[[\"country\", \"RegionSimple_Europe\"]].head()"
      ],
      "metadata": {
        "id": "PlYCU_5fXpJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Justification**\n",
        "\n",
        "The main models in this project do not rely on categorical predictors, but this example shows how a simple region dummy (`RegionSimple_Europe`) could be constructed.\n",
        "\n",
        "Such dummies could be used in robustness checks or extended models to control for broad regional differences (e.g. European vs non-European countries). For the core analysis, I keep the specification focused on the theoretically central predictors: pollution, income, urbanization, and health expenditure."
      ],
      "metadata": {
        "id": "TuwNGvG-Xtqw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "ruby"
        },
        "id": "ae84f8c3"
      },
      "source": [
        "\n",
        "### 2.3 Addressing Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "084161b5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Identify outliers using IQR method\n",
        "# Q1 = df['numeric_column'].quantile(0.25)\n",
        "# Q3 = df['numeric_column'].quantile(0.75)\n",
        "# IQR = Q3 - Q1\n",
        "# lower_bound = Q1 - 1.5 * IQR\n",
        "# upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Remove outliers??\n",
        "# df_no_outliers = df[(df['numeric_column'] >= lower_bound) & (df['numeric_column'] <= upper_bound)]\n",
        "\n",
        "# Or cap outliers at percentiles\n",
        "# df['numeric_column'] = df['numeric_column'].clip(lower=df['numeric_column'].quantile(0.05),\n",
        "#                                                  upper=df['numeric_column'].quantile(0.95))\n",
        "\n",
        "# Use Z-score to identify outliers\n",
        "# from scipy import stats\n",
        "# z_scores = np.abs(stats.zscore(df['numeric_column']))\n",
        "# df_no_outliers = df[z_scores < 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1907d58c"
      },
      "source": [
        "<!-- Your justification here. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8dc70b0"
      },
      "source": [
        "### 2.4 Variable Transformation/Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "884f26f8"
      },
      "outputs": [],
      "source": [
        "# Create new variables or transform existing ones\n",
        "# Example: Create interaction terms\n",
        "# df['interaction_var'] = df['var1'] * df['var2']\n",
        "\n",
        "# Example: Create categorical variables from continuous ones\n",
        "# df['category_var'] = pd.cut(df['continuous_var'], bins=3, labels=['Low', 'Medium', 'High'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5facea2f"
      },
      "source": [
        "<!-- Your justification here. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc106e73"
      },
      "source": [
        "## 3. Rigorous Assumption Checking and Interpretation\n",
        "This section presents the diagnostic checks for your final chosen model. For each assumption, provide the relevant plot or test, the code that generated it, and a clear interpretation of the result. Discuss any violations and their implications for your conclusions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dce20fe"
      },
      "source": [
        "### 3.1. Linearity and Homoscedasticity (Residuals vs. Fitted Plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35ba08f5"
      },
      "outputs": [],
      "source": [
        "# plt.scatter(results.fittedvalues, results.resid)\n",
        "# plt.xlabel('Fitted values')\n",
        "# plt.ylabel('Residuals')\n",
        "# plt.title('Residuals vs Fitted')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61510644"
      },
      "source": [
        "<!-- Your interpretation here. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a02b3eef"
      },
      "source": [
        "### 3.2. Normality of Residuals (Q-Q Plot & Histogram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2dd7c4a"
      },
      "outputs": [],
      "source": [
        "# stats.probplot(results.resid, dist=\"norm\", plot=plt)\n",
        "# plt.show()\n",
        "# plt.hist(results.resid, bins=30)\n",
        "# plt.title('Histogram of Residuals')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "388eb6f0"
      },
      "source": [
        "<!-- Your interpretation here. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39cdb940"
      },
      "source": [
        "### 3.3. Multicollinearity (VIF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7149a92c"
      },
      "outputs": [],
      "source": [
        "# X = df[['var1', 'var2', 'var3']] # replace with your predictors\n",
        "# vif_data = pd.DataFrame()\n",
        "# vif_data['feature'] = X.columns\n",
        "# vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "# print(vif_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "735d1b22"
      },
      "source": [
        "<!-- Your interpretation here. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5101c308"
      },
      "source": [
        "### 3.4. Influential Observations (Cook's Distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "522aeea9"
      },
      "outputs": [],
      "source": [
        "# influence = results.get_influence()\n",
        "# cooks = influence.cooks_distance[0]\n",
        "# print(pd.Series(cooks).sort_values(ascending=False).head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "281a9160"
      },
      "source": [
        "<!-- Your interpretation here. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbfc2b0e"
      },
      "source": [
        "## 4. Purposeful Multiple Linear Model Formulation\n",
        "Define the at least three distinct models you will estimate. For each model, provide the theoretical and/or empirical justification for its specification (i.e., why you included/excluded certain variables).\n",
        "\n",
        "**Model 1: Baseline Model**\n",
        "\n",
        "*Justification:* This model includes... because...\n",
        "\n",
        "**Model 2: Extended Model**\n",
        "\n",
        "*Justification:* This model adds... to test...\n",
        "\n",
        "**Model 3: Interaction/Alternative Model**\n",
        "\n",
        "*Justification:* This model incorporates an interaction term between... and... to examine..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d295097"
      },
      "source": [
        "## 5. Model Estimation, Comprehensive Evaluation, and Comparative Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6f37b6c"
      },
      "source": [
        "### 5.1. Model Estimation\n",
        "Present the full statsmodels summary output for each of your three models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ebb0f5b"
      },
      "source": [
        "**Model 1 Results:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa93e616"
      },
      "outputs": [],
      "source": [
        "# model1 = smf.ols('y ~ x1 + x2 + x3', data=df).fit()\n",
        "# print(model1.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7d6dfd0"
      },
      "source": [
        "<!-- Model 1 summary output -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "098f0924"
      },
      "source": [
        "**Model 2 Results:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb36dfd2"
      },
      "outputs": [],
      "source": [
        "# model2 = smf.ols('y ~ x1 + x2 + x3 + x4', data=df).fit()\n",
        "# print(model2.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96f4aae4"
      },
      "source": [
        "<!-- Model 2 summary output -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07d5aa53"
      },
      "source": [
        "**Model 3 Results:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b3fb511"
      },
      "outputs": [],
      "source": [
        "# model3 = smf.ols('y ~ x1 * x2 + x3', data=df).fit()\n",
        "# print(model3.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3db9b463"
      },
      "source": [
        "<!-- Model 3 summary output -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6da859d"
      },
      "source": [
        "### 5.2. Comparative Analysis\n",
        "Compare your models using appropriate metrics (e.g., Adjusted R-squared) and theoretical fit. Create a table summarizing key coefficients and fit statistics across models for easy comparison. Justify which model you consider to be the 'best' for answering your research question, considering parsimony, theoretical coherence, and statistical performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af5a8572"
      },
      "source": [
        "## 6. Substantive Conclusions and Limitations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ad4d0b4"
      },
      "source": [
        "### 6.1. Conclusions\n",
        "Summarize your key findings based on your chosen model. Directly address your research question and state whether your hypotheses were supported or not. Discuss the substantive and theoretical implications of your results in a political science context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c1e409c"
      },
      "source": [
        "### 6.2. Limitations\n",
        "\n",
        "Acknowledge the limitations of your study. Consider data limitations, potential violations of model assumptions, and issues of generalizability.\n"
      ]
    }
  ]
}